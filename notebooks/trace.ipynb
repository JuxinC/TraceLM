{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Import Self-written Functions\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "sys.path.append(src_dir)\n",
    "print(src_dir)\n",
    "\n",
    "from load_data.data_loader import load_data_from_db,remove_illegal_chars\n",
    "from clean_data.cleanCommitData import cleanCommitData\n",
    "from clean_data.cleanJiraData import cleanJiraData\n",
    "from clean_data.subsetAccordTime import subsetAccordTime\n",
    "from clean_data.checkValidityTrace import checkValidityTrace\n",
    "from clean_data.createCorpusFromDocumentList import createCorpusFromDocumentList\n",
    "\n",
    "from features_engineering.calculateTimeDifference import *\n",
    "from features_engineering.checkAuthorMatch import checkAuthorMatch\n",
    "\n",
    "from model_similarity.embedding_choice import *\n",
    "from model_similarity.createFittedTF_IDF import createFittedTF_IDF,calculateCosineSimilarity\n",
    "from model_similarity.wordToVec import calculate_word2vec_similarity\n",
    "from model_similarity.fastText import calculate_fasttext_similarity\n",
    "from model_similarity.sentenceTransformer import *\n",
    "from model_similarity.openAI import *\n",
    "\n",
    "\n",
    "#Display full value of a column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current project name\n",
    "project='derby'        #'derby','flink','hbase','kafka','pig','switchyard','teiid','zookeeper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Jira&SVN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate datasets dir\n",
    "intermediate_dir = f\"../data/intermediate/{project}\"\n",
    "os.makedirs(intermediate_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw JIRA & SVN data as a pandas dataframe\n",
    "#formal projects\n",
    "commit_df, issue_df, link_df = load_data_from_db(f\"../data/raw_data/SEOSS/dataverse_files/{project}.sqlite3\")\n",
    "\n",
    "#temp\n",
    "link_df.to_excel(excel_writer = f\"{intermediate_dir}/link_df.xlsx\", index = False)\n",
    "link_df.to_pickle(path= f\"{intermediate_dir}/link_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clean Raw Data - Commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data of the SVN table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "commit_df_clean = cleanCommitData(commit_df)\n",
    "\n",
    "#temp \n",
    "commit_df_clean.to_excel(excel_writer = f\"{intermediate_dir}/commit_df_clean.xlsx\", index = False)\n",
    "commit_df_clean.to_pickle(path= f\"{intermediate_dir}/commit_df_clean.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished cleaning after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean Raw Data - JIRA Data\n",
    "Clean the raw data of the Jira table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data sets\n",
    "jira_df_clean = cleanJiraData(issue_df)\n",
    "\n",
    "#temp \n",
    "jira_df_clean.to_pickle(path= f\"{intermediate_dir}/jira_df_clean.pkl\")\n",
    "jira_df_clean_ri = remove_illegal_chars(jira_df_clean)\n",
    "jira_df_clean_ri.to_excel(excel_writer = f\"{intermediate_dir}/jira_df_clean.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Subset according to the time interval \n",
    "To reduce the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "commit_df_clean,jira_df_clean,link_df=subsetAccordTime(commit_df_clean,jira_df_clean,link_df,interval=1000000)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "commit_df_clean.to_excel(excel_writer = f\"{intermediate_dir}/commit_df_clean.xlsx\", index = False)\n",
    "jira_df_clean_ri = remove_illegal_chars(jira_df_clean)\n",
    "jira_df_clean_ri.to_excel(excel_writer = f\"{intermediate_dir}/jira_df_clean.xlsx\", index = False)\n",
    "link_df.to_excel(excel_writer = f\"{intermediate_dir}/link_df.xlsx\", index = False)\n",
    "\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "commit_df_clean.to_pickle(path= f\"{intermediate_dir}/commit_df_clean.pkl\")\n",
    "jira_df_clean.to_pickle(path= f\"{intermediate_dir}/jira_df_clean.pkl\")\n",
    "link_df.to_pickle(path= f\"{intermediate_dir}/link_df.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished subset after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create Corpora\n",
    "Create the corpora for JIRA Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "jira_corpus_summary  = createCorpusFromDocumentList(jira_df_clean.Summary)\n",
    "jira_corpus_description = createCorpusFromDocumentList(jira_df_clean.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "jira_corpus_all = [i+\" \"+j for i,j in zip(jira_corpus_summary,\n",
    "                                          jira_corpus_description)]\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open(f\"{intermediate_dir}/jira_corpus_summary.pkl\", 'wb') as f:\n",
    "    pickle.dump(jira_corpus_summary, f)\n",
    "\n",
    "with open(f\"{intermediate_dir}/jira_corpus_description.pkl\", 'wb') as f:\n",
    "    pickle.dump(jira_corpus_description, f)\n",
    "\n",
    "with open(f\"{intermediate_dir}/jira_corpus_all.pkl\", 'wb') as f:\n",
    "    pickle.dump(jira_corpus_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the corpora for SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corpus for log messages\n",
    "svn_corpus_log = createCorpusFromDocumentList(commit_df_clean.Logs)\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open(f\"{intermediate_dir}/svn_corpus_log.pkl\", 'wb') as f:\n",
    "    pickle.dump(svn_corpus_log, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cartesian product JIRA x Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_df = jira_df_clean.merge(commit_df_clean, how='cross')\n",
    "print(cartesian_df.shape)\n",
    "\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_df = cartesian_df.drop(cartesian_df[cartesian_df.Jira_created_date > cartesian_df.Commit_date].index)\n",
    "print(cartesian_df.shape)\n",
    "#Create a pickle file for all intermediate datasets\n",
    "cartesian_df.to_pickle(path= f\"{intermediate_dir}/cartesian_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run line below to get cartesian df\n",
    "cartesian_df = pd.read_pickle(f'{intermediate_dir}/cartesian_df.pkl')\n",
    "\n",
    "print(cartesian_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features dir\n",
    "feature_dir = f\"../data/features/{project}\"\n",
    "os.makedirs(feature_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_set = set(zip(link_df[\"issue_id\"], link_df[\"commit_hash\"]))\n",
    "\n",
    "#Create new dataFrames for the time features\n",
    "labels_df = pd.DataFrame() \n",
    "\n",
    "#Create a column, which indicates which traces are valid.\n",
    "labels_df[\"is_valid\"] = cartesian_df.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.commit_hash,link_set), axis=1)\n",
    "\n",
    "print(\"Finished creating labels\")\n",
    "\n",
    "#Save labels\n",
    "labels_df.to_pickle(path= f\"{feature_dir}/labels_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Prepare model-Word2Vec,FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)  \n",
    "    text = re.sub(r'[{}()\\[\\]<>]', ' ', text)  \n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words and len(word) > 1)\n",
    "    return text\n",
    "\n",
    "projects = ['derby', 'flink', 'hbase', 'kafka', 'pig', 'switchyard', 'teiid', 'zookeeper']\n",
    "all_texts = []\n",
    "\n",
    "for project in projects:\n",
    "    intermediate_dir = f\"../data/intermediate/{project}\"\n",
    "    jira_path = os.path.join(intermediate_dir, \"jira_df_clean.pkl\")\n",
    "    commit_path = os.path.join(intermediate_dir, \"commit_df_clean.pkl\")\n",
    "\n",
    "   \n",
    "    jira_df = pd.read_pickle(jira_path)\n",
    "    commit_df = pd.read_pickle(commit_path)\n",
    "\n",
    "    \n",
    "    jira_texts = (\n",
    "        jira_df['summary'].fillna('') + ' ' + jira_df['description'].fillna('')\n",
    "    ).tolist()\n",
    "    commit_texts = commit_df['Message'].fillna('').tolist()\n",
    "\n",
    "    project_texts = jira_texts + commit_texts\n",
    "    all_texts.extend(project_texts)\n",
    "\n",
    "print(f\"Loaded texts from {len(projects)} projects. Total texts: {len(all_texts)}\")\n",
    "print(\"Done,all_texts\")\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in all_texts if text.strip()]\n",
    "tokenized_texts = [word_tokenize(text) for text in cleaned_texts]\n",
    "model_dir = f\"../data/models\"\n",
    "print(\"Done,tokenized_texts\")\n",
    "\n",
    "model_1   = Word2Vec(sentences=tokenized_texts, vector_size=300, window=5, min_count=10, workers=4,epochs=30, sg=1)  \n",
    "model_1.save(f\"{model_dir}/word2vec_trained.model\")\n",
    "print(\"Done,word2vec\")\n",
    "model_2 = FastText(sentences=tokenized_texts,vector_size=300, window=5, min_count=10, workers=4,epochs=30, sg=1)\n",
    "model_2.save(f\"{model_dir}/fasttext_trained.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained results.\n",
    "#features dir\n",
    "intermediate_dir = f\"../data/intermediate/{project}\"\n",
    "feature_dir = f\"../data/features/{project}\"\n",
    "#Load JIRA Corpora\n",
    "jira_corpus_summary = pd.read_pickle(f\"{intermediate_dir}/jira_corpus_summary.pkl\")\n",
    "jira_corpus_description = pd.read_pickle(f\"{intermediate_dir}/jira_corpus_description.pkl\")\n",
    "jira_corpus_all = pd.read_pickle(f\"{intermediate_dir}/jira_corpus_all.pkl\")\n",
    "\n",
    "#Load SVN corora\n",
    "svn_corpus_log = pd.read_pickle(f\"{intermediate_dir}/svn_corpus_log.pkl\")\n",
    "\n",
    "#Load clean datasets\n",
    "jira_df_clean = pd.read_pickle(f\"{intermediate_dir}/jira_df_clean.pkl\")\n",
    "svn_df_clean = pd.read_pickle(f\"{intermediate_dir}/commit_df_clean.pkl\")\n",
    "\n",
    "#load cartesian products JIRA x Commits\n",
    "cartesian_df = pd.read_pickle(f\"{intermediate_dir}/cartesian_df.pkl\")\n",
    "labels_df=pd.read_pickle(f\"{feature_dir}/labels_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Create Non-textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "features_process_related = pd.DataFrame() \n",
    "\n",
    "#check whether the assignee/reporter name (in jira) matches the author name (in svn)\n",
    "features_process_related['f1_assignee_is_commiter'] = cartesian_df.apply(lambda x: checkAuthorMatch(x.Author, x.Email,x.assignee,x.assignee_username,x.reporter,x.reporter_username), axis=1)\n",
    "\n",
    "#Calculate the time features for data Processing Dataset\n",
    "features_process_related['f2_timedif_issuecreation_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "features_process_related['f3_timedif_issueupdated_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "features_process_related['f4_timedif_issueresolved_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished data Processing\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "features_process_related.to_pickle(path= f\"{feature_dir}/features_process_related.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create Textual Features --different techs\n",
    "### 3.2.1 Create TF-IDF (VSM) for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "#Create new dataFrame\n",
    "features_information_retrieval = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer() for SVN\n",
    "\n",
    "svn_log_countvectorizer = CountVectorizer()\n",
    "svn_log_tfidf = createFittedTF_IDF(svn_log_countvectorizer, svn_corpus_log)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA - unigram\n",
    "jira_all_countvectorizer = CountVectorizer()\n",
    "jira_all_tfidf = createFittedTF_IDF(jira_all_countvectorizer, jira_corpus_all)\n",
    "\n",
    "jira_summary_countvectorizer = CountVectorizer()\n",
    "jira_summary_tfidf = createFittedTF_IDF(jira_summary_countvectorizer, jira_corpus_summary)\n",
    "\n",
    "jira_description_countvectorizer = CountVectorizer()\n",
    "jira_description_tfidf = createFittedTF_IDF(jira_description_countvectorizer, jira_corpus_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f5_log_and_summary_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                                                 svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "#features_information_retrieval.to_excel(f\"{feature_dir}/features_information_retrieval.xlsx\", index=False)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f5_log_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                    jira_summary_countvectorizer, \n",
    "                                                                                                                                    jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_information_retrieval['f5_log_and_summary'] = (\n",
    "    features_information_retrieval['f5_log_and_summary_log_as_query'] + \n",
    "    features_information_retrieval['f5_log_and_summary_summary_as_query']\n",
    ") / 2\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f6_log_and_description_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, \n",
    "                                                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                                                 svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f6_log_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, \n",
    "                                                                                                                                    jira_description_countvectorizer, \n",
    "                                                                                                                                    jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_information_retrieval['f6_log_and_description'] = (\n",
    "    features_information_retrieval['f6_log_and_description_log_as_query'] + \n",
    "    features_information_retrieval['f6_log_and_description_description_as_query']\n",
    ") / 2\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f7_log_and_jira_all_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                                                 svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f7_log_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                                    jira_all_countvectorizer, \n",
    "                                                                                                                                    jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_information_retrieval['f7_log_and_jira_all'] = (\n",
    "    features_information_retrieval['f7_log_and_jira_all_log_as_query'] + \n",
    "    features_information_retrieval['f7_log_and_jira_all_jira_all_as_query']\n",
    ") / 2\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= f\"{feature_dir}/features_information_retrieval.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Create WordtoVec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_word2vec = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_path = f\"../data/models/word2vec_trained.model\"\n",
    "model=Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordtoVec Features - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8  # Adjust batch size as needed to fit GPU memory\n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_word2vec[\"f5_log_and_summary\"] = calculate_word2vec_similarity(cartesian_df['summary'].tolist(), cartesian_df['Message'].tolist(), model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "os.makedirs(f\"{feature_dir}/word2vec\", exist_ok=True)\n",
    "features_word2vec.to_pickle(path=f\"{feature_dir}/word2vec/features_word2vec.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordtoVec Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8  # Adjust batch size as needed to fit GPU memory\n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_word2vec[\"f6_log_and_description\"] = calculate_word2vec_similarity(cartesian_df['description'].tolist(), cartesian_df['Message'].tolist(), model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "features_word2vec.to_pickle(path= f\"{feature_dir}/word2vec/features_word2vec.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordtoVec Features - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8  # Adjust batch size as needed to fit GPU memory\n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_word2vec[\"f7_log_and_jira_all\"] = calculate_word2vec_similarity(cartesian_df['jira_natual_text'].tolist(), cartesian_df['Message'].tolist(), model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "features_word2vec.to_pickle(path= f\"{feature_dir}/word2vec/features_word2vec.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Create FastText for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_fastText = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model_path = f\"../data/models/fasttext_trained.model\"\n",
    "model=FastText.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Features - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8  # Adjust batch size as needed to fit GPU memory\n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_fastText[\"f5_log_and_summary\"] = calculate_fasttext_similarity(cartesian_df['summary'].tolist(), cartesian_df['Message'].tolist(), model=model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "os.makedirs(f\"{feature_dir}/fast_text\", exist_ok=True)\n",
    "features_fastText.to_pickle(path= f\"{feature_dir}/fast_text/features_fast_text.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8 \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "\n",
    "features_fastText[\"f6_log_and_description\"] = calculate_fasttext_similarity(cartesian_df['description'].tolist(), cartesian_df['Message'].tolist(), model=model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "features_fastText.to_pickle(path= f\"{feature_dir}/fast_text/features_fast_text.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Features - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "# Split computation into smaller batches\n",
    "batch_size = 8  # Adjust batch size as needed to fit GPU memory\n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "\n",
    "features_fastText[\"f7_log_and_jira_all\"] = calculate_fasttext_similarity(cartesian_df['jira_natual_text'].tolist(), cartesian_df['Message'].tolist(), model=model,batch_size=batch_size)\n",
    "\n",
    "#Save results in pickle\n",
    "features_fastText.to_pickle(path= f\"{feature_dir}/fast_text/features_fast_text.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Create SentenceTransformer  embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_sentence_transformer = pd.DataFrame() \n",
    "#Create embedding path\n",
    "model_type='sentence_transformer'\n",
    "embeddings_dir = f\"{feature_dir}/{model_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary embedding (Jira)\n",
    "embedding_summary_df=process_jira_embeddings(jira_df_clean, 'summary', 'summary_embedding', embeddings_dir,model_type='sentence_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description embedding (Jira)\n",
    "embedding_description_df=process_jira_embeddings(jira_df_clean, 'description', 'description_embedding', embeddings_dir,model_type='sentence_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All: summary+description (Jira)\n",
    "embedding_all_df=process_jira_embeddings(jira_df_clean, 'jira_natual_text', 'jira_natual_text_embedding', embeddings_dir,model_type='sentence_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Message (SVN)\n",
    "embedding_message_df=process_svn_embeddings(svn_df_clean, 'Message', 'Message_embedding', embeddings_dir,model_type='sentence_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained embeddings.\n",
    "embedding_summary_df = pd.read_pickle(f\"{embeddings_dir}/embedding_summary.pkl\")\n",
    "embedding_description_df = pd.read_pickle(f\"{embeddings_dir}/embedding_description.pkl\")\n",
    "embedding_all_df = pd.read_pickle(f\"{embeddings_dir}/embedding_jira_natual_text.pkl\")\n",
    "embedding_message_df = pd.read_pickle(f\"{embeddings_dir}/embedding_Message.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentenceTransformer  - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_summary_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f5_log_and_summary(features_sentence_transformer,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentenceTransformer Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_description_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f6_log_and_description(features_sentence_transformer,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentenceTransformer - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_all_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f7_log_and_jira_all(features_sentence_transformer,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Create LLM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_openai = pd.DataFrame() \n",
    "#Create embedding path\n",
    "model_type='openai'\n",
    "embeddings_dir = f\"{feature_dir}/{model_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary embedding (Jira)\n",
    "embedding_summary_df=process_jira_embeddings(jira_df_clean, 'summary', 'summary_embedding', embeddings_dir,model_type='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description embedding (Jira)\n",
    "embedding_description_df=process_jira_embeddings(jira_df_clean, 'description', 'description_embedding', embeddings_dir,model_type='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All: summary+description (Jira)\n",
    "embedding_all_df=process_jira_embeddings(jira_df_clean, 'jira_natual_text', 'jira_natual_text_embedding', embeddings_dir,model_type='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Message (SVN)\n",
    "embedding_message_df=process_svn_embeddings(svn_df_clean, 'Message', 'Message_embedding', embeddings_dir,model_type='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained embeddings.\n",
    "embedding_summary_df = pd.read_pickle(f\"{embeddings_dir}/embedding_summary.pkl\")\n",
    "embedding_description_df = pd.read_pickle(f\"{embeddings_dir}/embedding_description.pkl\")\n",
    "embedding_all_df = pd.read_pickle(f\"{embeddings_dir}/embedding_jira_natual_text.pkl\")\n",
    "embedding_message_df = pd.read_pickle(f\"{embeddings_dir}/embedding_Message.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openAI  - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_summary_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f5_log_and_summary(features_openai,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openAI - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_description_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f6_log_and_description(features_openai,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openAI - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_embeddings = embedding_all_df.merge(embedding_message_df, how='cross')\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_embeddings = cartesian_embeddings.drop(cartesian_embeddings[cartesian_embeddings.Jira_created_date > cartesian_embeddings.Commit_date].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "f7_log_and_jira_all(features_openai,cartesian_embeddings,feature_dir,model_type)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Generate feature sets - Load and transform feature families needed for training\n",
    "Load features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Process-Related Features\n",
    "features_process_related = pd.read_pickle(f\"{feature_dir}/features_process_related.pkl\")\n",
    "\n",
    "#Load IR TF-IDF Features\n",
    "features_information_retrieval = pd.read_pickle(f\"{feature_dir}/features_information_retrieval.pkl\")\n",
    "\n",
    "#Load Word2Vec Features\n",
    "features_word2vec = pd.read_pickle(f\"{feature_dir}/features_word2vec.pkl\")\n",
    "\n",
    "#Load fastText Features\n",
    "features_fastText= pd.read_pickle(f\"{feature_dir}/features_fast_text.pkl\")\n",
    "\n",
    "#Load SentenceTransformer Features\n",
    "features_sentence_transformer= pd.read_pickle(f\"{feature_dir}/features_sentence_transformer.pkl\")\n",
    "\n",
    "#Load openAi Features\n",
    "features_openai= pd.read_pickle(f\"{feature_dir}/features_openai.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc, fbeta_score,\n",
    "    f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "feature_dir = f\"../data/features/{project}\"\n",
    "results_dir = f\"../data/results/{project}\"\n",
    "model_types = ['information_retrieval', 'fast_text', 'word2vec', 'sentence_transformer', 'openai']#[\n",
    "similarity_cols = ['f5_log_and_summary', 'f6_log_and_description', 'f7_log_and_jira_all']\n",
    "results_all = []\n",
    "def get_best_threshold(y_true, y_scores, beta_list=[1.0]):#[0.5, 1.0, 2.0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    # Add a maximum value to ensure alignment with precision/recall\n",
    "    thresholds = np.append(thresholds, 1.0)  \n",
    "\n",
    "    best_results = {}\n",
    "    for beta in beta_list:\n",
    "        f_beta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-8)\n",
    "        best_idx = np.argmax(f_beta)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "        best_results[beta] = {\n",
    "            'threshold': best_threshold,\n",
    "            'precision': precision[best_idx],\n",
    "            'recall': recall[best_idx],\n",
    "            f'F{beta}': f_beta[best_idx]\n",
    "        }\n",
    "    return best_results\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n====== Evaluating model: {model_type} ======\")\n",
    "\n",
    "    # Load features and labels\n",
    "    features_df = pd.read_pickle(f\"{feature_dir}/{model_type}/features_{model_type}.pkl\")\n",
    "    labels_df = pd.read_pickle(f\"{feature_dir}/labels_df.pkl\")\n",
    "\n",
    "\n",
    "    # Calculate the average similarity feature\n",
    "    features_df['avg_similarity'] = features_df[similarity_cols].mean(axis=1)\n",
    "    all_cols = similarity_cols + ['avg_similarity']\n",
    "\n",
    "    labels = labels_df\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_df[all_cols], labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    X_train= X_train.fillna(0)\n",
    "    X_test= X_test.fillna(0)\n",
    "\n",
    "    for col in all_cols:\n",
    "        print(f\"\\n--- Evaluating similarity feature: {col} ---\")\n",
    "        \n",
    "        # Best thresholds for F0.5, F1, F2\n",
    "        best_thresholds = get_best_threshold(y_train.values, X_train[col].values)\n",
    "\n",
    "        y_scores = X_test[col].values\n",
    "        y_true = y_test.values\n",
    "\n",
    "\n",
    "        for beta, result in best_thresholds.items():\n",
    "            threshold = result['threshold']\n",
    "            preds = (y_scores >= threshold).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            prec = precision_score(y_true, preds)\n",
    "            rec = recall_score(y_true, preds)\n",
    "            f1 = f1_score(y_true, preds)\n",
    "            f2 = fbeta_score(y_true, preds, beta=2)\n",
    "            f05 = fbeta_score(y_true, preds, beta=0.5)\n",
    "            ap_score = average_precision_score(y_true, preds)\n",
    "            roc_auc = roc_auc_score(y_true, preds)\n",
    "\n",
    "            results_all.append({\n",
    "                'model_type': model_type,\n",
    "                'similarity_feature': col,\n",
    "                'F_type': f'F{beta}',\n",
    "                'threshold': round(threshold, 4),\n",
    "                'precision': round(prec, 4),\n",
    "                'recall': round(rec, 4),\n",
    "                'accuracy': round(acc, 4),\n",
    "                'F1': round(f1, 4),\n",
    "                'F2':round(f2,4),\n",
    "                'F0.5':round(f05,4),\n",
    "                'avg_precision_score': round(ap_score, 4),\n",
    "                #f'F{beta}': round(result[f'F{beta}'], 4),\n",
    "                'ROC_AUC': round(roc_auc, 4),\n",
    "                'PR_AUC': round(ap_score, 4),\n",
    "                \n",
    "            })\n",
    "\n",
    "        # Optional: Draw a PR curve\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_scores)\n",
    "        plt.plot(recall_curve, precision_curve, label=f\"{col} (PR AUC={ap_score:.2f})\")\n",
    "\n",
    "    plt.title(f\"PR Curves - {model_type}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Save results to Excel\n",
    "results_df = pd.DataFrame(results_all)\n",
    "results_df.to_excel(f\"{results_dir}/local_rq1_test_results.xlsx\", index=False)\n",
    "print(f\"All model evaluation results have been saved to: {results_dir}/local_rq1_test_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 RQ2 & RQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models dir\n",
    "models_dir = f\"../data/models/{project}\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(models_dir)\n",
    "\n",
    "# Load similarity features \n",
    "model_type= 'sentence_transformer' #''information_retrieval', 'fast_text', 'word2vec', 'sentence_transformer', 'openai'\n",
    "#results dir\n",
    "results_dir = f\"../data/results/{project}/all/{model_type}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(results_dir)\n",
    "\n",
    "#load data\n",
    "features_similarity = pd.read_pickle(f\"{feature_dir}/features_{model_type}.pkl\")\n",
    "\n",
    "features_process = pd.read_pickle(f\"{feature_dir}/features_process_related.pkl\")\n",
    "\n",
    "labels = pd.read_pickle(f\"{feature_dir}/labels_df.pkl\")\n",
    "features_similarity = features_similarity.reset_index(drop=True)\n",
    "features_process = features_process.reset_index(drop=True)\n",
    "labels=labels.reset_index(drop=True)\n",
    "print(features_similarity.shape,features_process.shape,labels.shape)\n",
    "#concat data\n",
    "process_cols = ['f1_assignee_is_commiter', 'f2_timedif_issuecreation_and_commitcreation', \n",
    "                'f3_timedif_issueupdated_and_commitcreation', 'f4_timedif_issueresolved_and_commitcreation']\n",
    "similarity_cols = ['f5_log_and_summary', 'f6_log_and_description', 'f7_log_and_jira_all']\n",
    "\n",
    "features_df = pd.concat([features_similarity[similarity_cols], features_process[process_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models dir\n",
    "models_dir = f\"../data/models/{project}\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load similarity features \n",
    "model_type= 'openai' #''information_retrieval', 'fast_text', 'word2vec', 'sentence_transformer', 'openai'\n",
    "#results dir\n",
    "results_dir = f\"../data/results/{project}/similarity/{model_type}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#load data\n",
    "features_similarity = pd.read_pickle(f\"{feature_dir}/features_{model_type}.pkl\")\n",
    "labels = pd.read_pickle(f\"{feature_dir}/labels_df.pkl\")\n",
    "\n",
    "features_similarity = features_similarity.reset_index(drop=True)\n",
    "labels=labels.reset_index(drop=True)\n",
    "\n",
    "#print(features_similarity.shape,labels.shape)\n",
    "#concat data\n",
    "\n",
    "similarity_cols = ['f5_log_and_summary', 'f6_log_and_description', 'f7_log_and_jira_all']\n",
    "features_df = features_similarity[similarity_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models dir\n",
    "models_dir = f\"../data/models/{project}\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "#results dir\n",
    "results_dir = f\"../data/results/{project}/process\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#load data\n",
    "features_process = pd.read_pickle(f\"{feature_dir}/features_process_related.pkl\")\n",
    "labels = pd.read_pickle(f\"{feature_dir}/labels_df.pkl\")\n",
    "\n",
    "features_process = features_process.reset_index(drop=True)\n",
    "labels=labels.reset_index(drop=True)\n",
    "\n",
    "print(features_process.shape,labels.shape)\n",
    "#concat data\n",
    "\n",
    "process_cols = ['f1_assignee_is_commiter', 'f2_timedif_issuecreation_and_commitcreation', \n",
    "                'f3_timedif_issueupdated_and_commitcreation', 'f4_timedif_issueresolved_and_commitcreation']\n",
    "features_df =features_process[process_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the NaN to 0\n",
    "features_all_df = features_df.fillna(0)\n",
    "feature_name_df = list(features_all_df.columns)\n",
    "#Transform pandas data frame into numpy arrays\n",
    "features_all_array = np.array(features_all_df)\n",
    "labels_array = np.array(labels[\"is_valid\"])\n",
    "print(feature_name_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "\n",
    "#Method to show the different model evaluation metrics\n",
    "def showModelPerformance(trainedModel, testFeatures, testLabels):#, threshold=0.5\n",
    "    # Use the fitted model to predict the labels of the test set\n",
    "    predictionLabels = trainedModel.predict(testFeatures)\n",
    "    \n",
    "    #Calculate the different metrics for the test vs predicted labels\n",
    "    accuracyValue = accuracy_score(testLabels.astype(bool), predictionLabels)\n",
    "    precisionValue = precision_score(testLabels.astype(bool), predictionLabels, average='binary')\n",
    "    f1Value = f1_score(testLabels.astype(bool), predictionLabels)\n",
    "    f2Value = fbeta_score(testLabels.astype(bool), predictionLabels, beta=2.0)\n",
    "    f05Value = fbeta_score(testLabels.astype(bool), predictionLabels, beta=0.5)\n",
    "    recallValue = recall_score(testLabels.astype(bool), predictionLabels)\n",
    "    averagePrecisionValue = average_precision_score(testLabels.astype(bool), predictionLabels)\n",
    "    \n",
    "    #Create a dataframe to output all evaluation metrics in\n",
    "    performanceData = {'Accuracy':  [accuracyValue],\n",
    "                       'Precision': [precisionValue],\n",
    "                       'Recall': [recallValue],\n",
    "                       'F1': [f1Value],\n",
    "                       'F2': [f2Value],\n",
    "                       'F0.5': [f05Value],\n",
    "                       'Average Precision': [averagePrecisionValue]\n",
    "                      }\n",
    "    performanceDf = pd.DataFrame(performanceData)\n",
    "    return(performanceDf)\n",
    "\n",
    "#Method to define the Pipeline steps based on the given rebalancing strategy and classification algorithm\n",
    "def define_steps(rebalancing_strategy, classification_algorithm):\n",
    "    steps = None\n",
    "    if(rebalancing_strategy == 'none'):\n",
    "        if(classification_algorithm == 'random_forests'):\n",
    "            steps = [['classifier', RandomForestClassifier(n_jobs=-1)]]\n",
    "        elif (classification_algorithm == 'xg_boost'):\n",
    "            steps = [['classifier', xgb.XGBClassifier(n_jobs=-1)]]\n",
    "            return(steps)\n",
    "\n",
    "    elif(rebalancing_strategy == 'over'):\n",
    "        if(classification_algorithm == 'random_forests'):\n",
    "            steps = [['smote', SMOTE()],\n",
    "                    ['classifier', RandomForestClassifier(n_jobs=-1)]]\n",
    "        elif (classification_algorithm == 'xg_boost'):\n",
    "            steps = [['smote', SMOTE()],\n",
    "                    ['classifier', xgb.XGBClassifier(n_jobs=-1)]]\n",
    "\n",
    "    elif(rebalancing_strategy == 'under'):\n",
    "        if(classification_algorithm == 'random_forests'):\n",
    "            steps = [['under', RandomUnderSampler()],\n",
    "                    ['classifier', RandomForestClassifier(n_jobs=-1)]]\n",
    "        elif (classification_algorithm == 'xg_boost'):\n",
    "            steps = [['under', RandomUnderSampler()],\n",
    "                    ['classifier', xgb.XGBClassifier(n_jobs=-1)]]\n",
    "\n",
    "    elif(rebalancing_strategy == '5050'):\n",
    "        if(classification_algorithm == 'random_forests'):\n",
    "            steps = [['smote', SMOTE(sampling_strategy = 0.5)],\n",
    "                    ['under', RandomUnderSampler()],\n",
    "                    ['classifier', RandomForestClassifier(n_jobs=-1)]]\n",
    "        elif (classification_algorithm == 'xg_boost'):\n",
    "            steps = [['smote', SMOTE(sampling_strategy = 0.5)],\n",
    "                    ['under', RandomUnderSampler()],\n",
    "                    ['classifier', xgb.XGBClassifier(n_jobs=-1)]]\n",
    "\n",
    "\n",
    "    return steps\n",
    "\n",
    "def get_param_space(algorithm_name):\n",
    "    if algorithm_name == \"random_forests\":\n",
    "        return {\n",
    "            'classifier__n_estimators': [],\n",
    "            'classifier__max_depth': [],\n",
    "            'classifier__min_samples_split': []\n",
    "        }\n",
    "    elif algorithm_name == \"xg_boost\":\n",
    "        return {\n",
    "            'classifier__n_estimators': [],\n",
    "            'classifier__learning_rate': [],\n",
    "            'classifier__max_depth': []\n",
    "        }\n",
    "\n",
    "\n",
    "#Method to generate the f1, f2, f0.5, accuracy, precision, recall, and average precision\n",
    "def generate_evaluation_metrics(rebalancing_strategy, classification_algorithm, data, labels, is_normalized, n_runs, feature_names):\n",
    "    #Create a dataframe to append to the results of each individual run\n",
    "    evaluation_df = pd.DataFrame(\n",
    "    {\n",
    "        'Accuracy':  [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': [],\n",
    "        'F2': [],\n",
    "        'F0.5': [],\n",
    "        'Average Precision': []\n",
    "    })\n",
    "    \n",
    "    #Create a np array to put the importances per feature in\n",
    "    importance_array = np.empty(shape=(n_runs, data.shape[1])) #85 data.shape[1]\n",
    "    \n",
    "    #Perform the described pipeline steps to produce the results for the defined number of runs\n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                        labels,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=labels)\n",
    "        \n",
    "        #Set the pipeline steps according to the defined rebalancing strategy and classification algorithm\n",
    "        steps = define_steps(rebalancing_strategy, classification_algorithm)\n",
    "        \n",
    "        #Create the pipeline\n",
    "        model_pipeline = Pipeline(steps=steps)\n",
    "        \n",
    "        space_empty = dict()    \n",
    "        \n",
    "        #hyper-parameters\n",
    "        space = get_param_space(classification_algorithm)\n",
    "\n",
    "        stratified_kfold = StratifiedKFold(n_splits=10,shuffle=True)           \n",
    "        randomized_search = RandomizedSearchCV(\n",
    "            estimator=model_pipeline,\n",
    "            param_distributions=space,\n",
    "            n_iter=10,  # The number of search rounds\n",
    "            n_jobs=1, #-1\n",
    "            cv=stratified_kfold,\n",
    "            scoring='f1',\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"model_pipeline\",model_pipeline)\n",
    "\n",
    "\n",
    "        print(f\"\\n====== The {i+1}th round of model parameter adjustment... ======\")\n",
    "        randomized_search.fit(X_train, y_train)\n",
    "        best_params = randomized_search.best_params_\n",
    "        print(\"Best parameters:\", best_params)\n",
    "\n",
    "        steps = define_steps(rebalancing_strategy, classification_algorithm)\n",
    "        classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items()}\n",
    "\n",
    "        for idx, (step_name, estimator) in enumerate(steps):\n",
    "            if step_name == 'classifier':\n",
    "                if classification_algorithm == \"random_forests\":\n",
    "                    steps[idx][1] = RandomForestClassifier(**classifier_params)\n",
    "                elif classification_algorithm == \"xg_boost\":\n",
    "                    steps[idx][1] = xgb.XGBClassifier(**classifier_params)\n",
    "\n",
    "        model = Pipeline(steps=steps)\n",
    "        \n",
    "        #Fit the model on the training data\n",
    "        fitted_model = model.fit(X_train, y_train)\n",
    "        \n",
    "        #Evaluate the fitted model\n",
    "        fitted_model_evaluation_df = showModelPerformance(trainedModel = fitted_model, \n",
    "                         testFeatures = X_test, \n",
    "                         testLabels = y_test)     #,threshold=0.3\n",
    "        fitted_model_evaluation_df['Best Params'] = str(best_params)\n",
    "        #Add the evaluation of the current run to the results of the previous runs\n",
    "        evaluation_df = pd.concat([evaluation_df,\n",
    "                                   fitted_model_evaluation_df])\n",
    "        \n",
    "        \n",
    "        #Find the feature importances of the fitted model\n",
    "        if(classification_algorithm == \"light_gbm\"):\n",
    "            #current_importances = fitted_model.best_estimator_._final_estimator.booster_.feature_importance(importance_type='gain')\n",
    "            current_importances = fitted_model.named_steps['classifier'].booster_.feature_importance(importance_type='gain')\n",
    "        else:\n",
    "            #current_importances = fitted_model.best_estimator_._final_estimator.feature_importances_\n",
    "            current_importances = fitted_model.named_steps['classifier'].feature_importances_\n",
    "        #Add the feature importances of the current fitted model to the results of the previous runs\n",
    "\n",
    "        importance_array[i] = current_importances  \n",
    "    #Set the index as the run number\n",
    "    evaluation_df = evaluation_df.reset_index(drop = True)\n",
    "    evaluation_df.index += 1 \n",
    "    evaluation_df.index.name = \"run\"\n",
    "    \n",
    "    #Output the evaluation data to a csv file\n",
    "    evaluation_df.to_csv(f\"{results_dir}/{classification_algorithm}_{rebalancing_strategy}_{n_runs}_fnew_results.csv\")\n",
    "    \n",
    "    #Transform the importance array to a data frame\n",
    "    importance_df = pd.DataFrame(data=importance_array, \n",
    "                                 columns= feature_names, \n",
    "                                 index=list(range(1, n_runs +1)))\n",
    "    \n",
    "    #Set the index as the run number\n",
    "    importance_df.index.name = \"run\"\n",
    "    \n",
    "    #Output the importance data to a csv file\n",
    "    importance_df.to_csv(f\"{results_dir}/{classification_algorithm}_{rebalancing_strategy}_{n_runs}_fnew_feature_importance_results.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_forests\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 10)#10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XG Boost\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
